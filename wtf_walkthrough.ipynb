{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hahdawg/opt/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_convolution_layer(inputs, output_units, convolution_width, causal=False, dilation_rate=[1], bias=True,\n",
    "                               activation=None, dropout=None, scope='temporal-convolution-layer', reuse=False):\n",
    "    \"\"\"\n",
    "    Convolution over the temporal axis of sequence data.\n",
    "    Args:\n",
    "        inputs: Tensor of shape [batch size, max sequence length, input_units].\n",
    "        output_units: Output channels for convolution.\n",
    "        convolution_width: Number of timesteps to use in convolution.\n",
    "        causal: Output at timestep t is a function of inputs at or before timestep t.\n",
    "        dilation_rate:  Dilation rate along temporal axis.\n",
    "    Returns:\n",
    "        Tensor of shape [batch size, max sequence length, output_units].\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        if causal:\n",
    "            shift = int(convolution_width / 2) + int(int(dilation_rate[0] - 1) / 2)\n",
    "            pad = tf.zeros([tf.shape(inputs)[0], shift, inputs.shape.as_list()[2]])\n",
    "            inputs = tf.concat([pad, inputs], axis=1)\n",
    "\n",
    "        W = tf.get_variable(\n",
    "            name='weights',\n",
    "            initializer=tf.random_normal_initializer(\n",
    "                mean=0,\n",
    "                stddev=1.0 / tf.sqrt(float(convolution_width)*float(shape(inputs, 2)))\n",
    "            ),\n",
    "            shape=[convolution_width, shape(inputs, 2), output_units]\n",
    "        )\n",
    "\n",
    "        z = tf.nn.convolution(inputs, W, padding='SAME', dilation_rate=dilation_rate)\n",
    "        if bias:\n",
    "            b = tf.get_variable(\n",
    "                name='biases',\n",
    "                initializer=tf.constant_initializer(),\n",
    "                shape=[output_units]\n",
    "            )\n",
    "            z = z + b\n",
    "        z = activation(z) if activation else z\n",
    "        z = tf.nn.dropout(z, dropout) if dropout is not None else z\n",
    "        z = z[:, :-shift, :] if causal else z\n",
    "        return z\n",
    "\n",
    "\n",
    "def time_distributed_dense_layer(inputs, output_units, bias=True, activation=None, batch_norm=None,\n",
    "                                 dropout=None, scope='time-distributed-dense-layer', reuse=False):\n",
    "    \"\"\"\n",
    "    Applies a shared dense layer to each timestep of a tensor of shape [batch_size, max_seq_len, input_units]\n",
    "    to produce a tensor of shape [batch_size, max_seq_len, output_units].\n",
    "    Args:\n",
    "        inputs: Tensor of shape [batch size, max sequence length, ...].\n",
    "        output_units: Number of output units.\n",
    "        activation: activation function.\n",
    "        dropout: dropout keep prob.\n",
    "    Returns:\n",
    "        Tensor of shape [batch size, max sequence length, output_units].\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        W = tf.get_variable(\n",
    "            name='weights',\n",
    "            initializer=tf.random_normal_initializer(mean=0.0, stddev=1.0 / float(shape(inputs, -1))),\n",
    "            shape=[shape(inputs, -1), output_units]\n",
    "        )\n",
    "        z = tf.einsum('ijk,kl->ijl', inputs, W)\n",
    "        if bias:\n",
    "            b = tf.get_variable(\n",
    "                name='biases',\n",
    "                initializer=tf.constant_initializer(),\n",
    "                shape=[output_units]\n",
    "            )\n",
    "            z = z + b\n",
    "\n",
    "        if batch_norm is not None:\n",
    "            z = tf.layers.batch_normalization(z, training=batch_norm, reuse=reuse)\n",
    "\n",
    "        z = activation(z) if activation else z\n",
    "        z = tf.nn.dropout(z, dropout) if dropout is not None else z\n",
    "        return z\n",
    "\n",
    "\n",
    "def shape(tensor, dim=None):\n",
    "    \"\"\"Get tensor shape/dimension as list/int\"\"\"\n",
    "    if dim is None:\n",
    "        return tensor.shape.as_list()\n",
    "    else:\n",
    "        return tensor.shape.as_list()[dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(x, features):\n",
    "    x = tf.concat([x, features], axis=2)\n",
    "\n",
    "    inputs = time_distributed_dense_layer(\n",
    "        inputs=x,\n",
    "        output_units=self.residual_channels,\n",
    "        activation=tf.nn.tanh,\n",
    "        scope='x-proj-encode'\n",
    "    )\n",
    "\n",
    "    skip_outputs = []\n",
    "    conv_inputs = [inputs]\n",
    "    for i, (dilation, filter_width) in enumerate(zip(self.dilations, self.filter_widths)):\n",
    "        dilated_conv = temporal_convolution_layer(\n",
    "            inputs=inputs,\n",
    "            output_units=2*self.residual_channels,\n",
    "            convolution_width=filter_width,\n",
    "            causal=True,\n",
    "            dilation_rate=[dilation],\n",
    "            scope='dilated-conv-encode-{}'.format(i)\n",
    "        )\n",
    "        conv_filter, conv_gate = tf.split(dilated_conv, 2, axis=2)\n",
    "        dilated_conv = tf.nn.tanh(conv_filter)*tf.nn.sigmoid(conv_gate)\n",
    "\n",
    "        outputs = time_distributed_dense_layer(\n",
    "            inputs=dilated_conv,\n",
    "            output_units=self.skip_channels + self.residual_channels,\n",
    "            scope='dilated-conv-proj-encode-{}'.format(i)\n",
    "        )\n",
    "        skips, residuals = tf.split(outputs, [self.skip_channels, self.residual_channels], axis=2)\n",
    "\n",
    "        inputs += residuals\n",
    "        conv_inputs.append(inputs)\n",
    "        skip_outputs.append(skips)\n",
    "\n",
    "    skip_outputs = tf.nn.relu(tf.concat(skip_outputs, axis=2))\n",
    "    h = time_distributed_dense_layer(skip_outputs, 128, scope='dense-encode-1', activation=tf.nn.relu)\n",
    "    y_hat = time_distributed_dense_layer(h, 1, scope='dense-encode-2')\n",
    "\n",
    "    return y_hat, conv_inputs[:-1]\n",
    "\n",
    "\n",
    "def initialize_decode_params(x, features):\n",
    "    x = tf.concat([x, features], axis=2)\n",
    "\n",
    "    inputs = time_distributed_dense_layer(\n",
    "        inputs=x,\n",
    "        output_units=self.residual_channels,\n",
    "        activation=tf.nn.tanh,\n",
    "        scope='x-proj-decode'\n",
    "    )\n",
    "\n",
    "    skip_outputs = []\n",
    "    conv_inputs = [inputs]\n",
    "    for i, (dilation, filter_width) in enumerate(zip(self.dilations, self.filter_widths)):\n",
    "        dilated_conv = temporal_convolution_layer(\n",
    "            inputs=inputs,\n",
    "            output_units=2*self.residual_channels,\n",
    "            convolution_width=filter_width,\n",
    "            causal=True,\n",
    "            dilation_rate=[dilation],\n",
    "            scope='dilated-conv-decode-{}'.format(i)\n",
    "        )\n",
    "        conv_filter, conv_gate = tf.split(dilated_conv, 2, axis=2)\n",
    "        dilated_conv = tf.nn.tanh(conv_filter)*tf.nn.sigmoid(conv_gate)\n",
    "\n",
    "        outputs = time_distributed_dense_layer(\n",
    "            inputs=dilated_conv,\n",
    "            output_units=self.skip_channels + self.residual_channels,\n",
    "            scope='dilated-conv-proj-decode-{}'.format(i)\n",
    "        )\n",
    "        skips, residuals = tf.split(outputs, [self.skip_channels, self.residual_channels], axis=2)\n",
    "\n",
    "        inputs += residuals\n",
    "        conv_inputs.append(inputs)\n",
    "        skip_outputs.append(skips)\n",
    "\n",
    "    skip_outputs = tf.nn.relu(tf.concat(skip_outputs, axis=2))\n",
    "    h = time_distributed_dense_layer(skip_outputs, 128, scope='dense-decode-1', activation=tf.nn.relu)\n",
    "    y_hat = time_distributed_dense_layer(h, 1, scope='dense-decode-2')\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HelperClass(object):\n",
    "    pass\n",
    "\n",
    "self = HelperClass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "batch_size = 8\n",
    "seq_len = 100\n",
    "self.residual_channels = 32\n",
    "self.skip_channels = 32\n",
    "self.num_decode_steps = 10\n",
    "self.decode_len = 64\n",
    "num_features = 7\n",
    "self.dilations = [2**i for i in range(8)]*3\n",
    "self.filter_widths = [2 for i in range(8)]*3\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[batch_size, seq_len])\n",
    "x = tf.expand_dims(x, 2)\n",
    "self.encode_len = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "features = tf.placeholder(tf.float32, shape=[batch_size, seq_len, num_features])\n",
    "\n",
    "y_hat_encode, conv_inputs = encode(x, features)\n",
    "initialize_decode_params(x, features);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_shape(xs, name):\n",
    "    print \"{0}.shape = {1}\".format(name, xs.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "cin = [[[\"c{0}{1}{2}\".format(i, j, k) for k in xrange(3)]\n",
    "       for j in xrange(200)]\n",
    "       for i in xrange(batch_size)]\n",
    "cin = np.array(cin)\n",
    "\n",
    "dilation = 4\n",
    "\n",
    "encode_len = 100 + np.arange(batch_size)\n",
    "batch_idx = np.arange(batch_size)\n",
    "batch_idx = np.tile(batch_idx[:, np.newaxis], (1, dilation))\n",
    "\n",
    "qbt = encode_len - dilation - 1\n",
    "temporal_idx = qbt[:, np.newaxis] + np.arange(dilation).reshape(1, -1)\n",
    "\n",
    "idx = np.c_[batch_idx.reshape(-1, 1), temporal_idx.reshape(-1, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0]\n",
      " [1 1 1 1]\n",
      " [2 2 2 2]\n",
      " [3 3 3 3]\n",
      " [4 4 4 4]\n",
      " [5 5 5 5]\n",
      " [6 6 6 6]\n",
      " [7 7 7 7]]\n"
     ]
    }
   ],
   "source": [
    "print batch_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 95  96  97  98]\n",
      " [ 96  97  98  99]\n",
      " [ 97  98  99 100]\n",
      " [ 98  99 100 101]\n",
      " [ 99 100 101 102]\n",
      " [100 101 102 103]\n",
      " [101 102 103 104]\n",
      " [102 103 104 105]]\n"
     ]
    }
   ],
   "source": [
    "print temporal_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0, 95],\n",
       "       [ 0, 96],\n",
       "       [ 0, 97],\n",
       "       [ 0, 98],\n",
       "       [ 1, 96],\n",
       "       [ 1, 97],\n",
       "       [ 1, 98],\n",
       "       [ 1, 99],\n",
       "       [ 2, 97],\n",
       "       [ 2, 98]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================\n",
      "dilation = 1\n",
      "conv_input.shape = (8, 100, 32)\n",
      "batch_idx.shape = (8,)\n",
      "temporal_idx.shape = (8,)\n",
      "idx.shape = (8, 2)\n",
      "slices.shape = (8, 32)\n",
      "slices.shape = (8, 1, 32)\n",
      "len(layer_ta) = 11\n",
      "===========================================================================\n",
      "dilation = 2\n",
      "conv_input.shape = (8, 100, 32)\n",
      "batch_idx.shape = (16,)\n",
      "temporal_idx.shape = (16,)\n",
      "idx.shape = (16, 2)\n",
      "slices.shape = (16, 32)\n",
      "slices.shape = (8, 2, 32)\n",
      "len(layer_ta) = 12\n",
      "===========================================================================\n",
      "dilation = 4\n",
      "conv_input.shape = (8, 100, 32)\n",
      "batch_idx.shape = (32,)\n",
      "temporal_idx.shape = (32,)\n",
      "idx.shape = (32, 2)\n",
      "slices.shape = (32, 32)\n",
      "slices.shape = (8, 4, 32)\n",
      "len(layer_ta) = 14\n",
      "===========================================================================\n",
      "dilation = 8\n",
      "conv_input.shape = (8, 100, 32)\n",
      "batch_idx.shape = (64,)\n",
      "temporal_idx.shape = (64,)\n",
      "idx.shape = (64, 2)\n",
      "slices.shape = (64, 32)\n",
      "slices.shape = (8, 8, 32)\n",
      "len(layer_ta) = 18\n",
      "===========================================================================\n",
      "dilation = 16\n",
      "conv_input.shape = (8, 100, 32)\n",
      "batch_idx.shape = (128,)\n",
      "temporal_idx.shape = (128,)\n",
      "idx.shape = (128, 2)\n",
      "slices.shape = (128, 32)\n",
      "slices.shape = (8, 16, 32)\n",
      "len(layer_ta) = 26\n",
      "===========================================================================\n",
      "dilation = 32\n",
      "conv_input.shape = (8, 100, 32)\n",
      "batch_idx.shape = (256,)\n",
      "temporal_idx.shape = (256,)\n",
      "idx.shape = (256, 2)\n",
      "slices.shape = (256, 32)\n",
      "slices.shape = (8, 32, 32)\n",
      "len(layer_ta) = 42\n",
      "===========================================================================\n",
      "dilation = 64\n",
      "conv_input.shape = (8, 100, 32)\n",
      "batch_idx.shape = (512,)\n",
      "temporal_idx.shape = (512,)\n",
      "idx.shape = (512, 2)\n",
      "slices.shape = (512, 32)\n",
      "slices.shape = (8, 64, 32)\n",
      "len(layer_ta) = 74\n",
      "===========================================================================\n",
      "dilation = 128\n",
      "conv_input.shape = (8, 100, 32)\n",
      "batch_idx.shape = (1024,)\n",
      "temporal_idx.shape = (1024,)\n",
      "idx.shape = (1024, 2)\n",
      "slices.shape = (1024, 32)\n",
      "slices.shape = (8, 128, 32)\n",
      "len(layer_ta) = 138\n",
      "===========================================================================\n",
      "dilation = 1\n",
      "conv_input.shape = (8, 100, 32)\n",
      "batch_idx.shape = (8,)\n",
      "temporal_idx.shape = (8,)\n",
      "idx.shape = (8, 2)\n",
      "slices.shape = (8, 32)\n",
      "slices.shape = (8, 1, 32)\n",
      "len(layer_ta) = 11\n",
      "===========================================================================\n",
      "dilation = 2\n",
      "conv_input.shape = (8, 100, 32)\n",
      "batch_idx.shape = (16,)\n",
      "temporal_idx.shape = (16,)\n",
      "idx.shape = (16, 2)\n",
      "slices.shape = (16, 32)\n",
      "slices.shape = (8, 2, 32)\n",
      "len(layer_ta) = 12\n",
      "===========================================================================\n",
      "dilation = 4\n",
      "conv_input.shape = (8, 100, 32)\n",
      "batch_idx.shape = (32,)\n",
      "temporal_idx.shape = (32,)\n",
      "idx.shape = (32, 2)\n",
      "slices.shape = (32, 32)\n",
      "slices.shape = (8, 4, 32)\n",
      "len(layer_ta) = 14\n",
      "===========================================================================\n",
      "dilation = 8\n",
      "conv_input.shape = (8, 100, 32)\n",
      "batch_idx.shape = (64,)\n",
      "temporal_idx.shape = (64,)\n",
      "idx.shape = (64, 2)\n",
      "slices.shape = (64, 32)\n",
      "slices.shape = (8, 8, 32)\n",
      "len(layer_ta) = 18\n",
      "===========================================================================\n",
      "dilation = 16\n",
      "conv_input.shape = (8, 100, 32)\n",
      "batch_idx.shape = (128,)\n",
      "temporal_idx.shape = (128,)\n",
      "idx.shape = (128, 2)\n",
      "slices.shape = (128, 32)\n",
      "slices.shape = (8, 16, 32)\n",
      "len(layer_ta) = 26\n",
      "===========================================================================\n",
      "dilation = 32\n",
      "conv_input.shape = (8, 100, 32)\n",
      "batch_idx.shape = (256,)\n",
      "temporal_idx.shape = (256,)\n",
      "idx.shape = (256, 2)\n",
      "slices.shape = (256, 32)\n",
      "slices.shape = (8, 32, 32)\n",
      "len(layer_ta) = 42\n",
      "===========================================================================\n",
      "dilation = 64\n",
      "conv_input.shape = (8, 100, 32)\n",
      "batch_idx.shape = (512,)\n",
      "temporal_idx.shape = (512,)\n",
      "idx.shape = (512, 2)\n",
      "slices.shape = (512, 32)\n",
      "slices.shape = (8, 64, 32)\n",
      "len(layer_ta) = 74\n",
      "===========================================================================\n",
      "dilation = 128\n",
      "conv_input.shape = (8, 100, 32)\n",
      "batch_idx.shape = (1024,)\n",
      "temporal_idx.shape = (1024,)\n",
      "idx.shape = (1024, 2)\n",
      "slices.shape = (1024, 32)\n",
      "slices.shape = (8, 128, 32)\n",
      "len(layer_ta) = 138\n",
      "===========================================================================\n",
      "dilation = 1\n",
      "conv_input.shape = (8, 100, 32)\n",
      "batch_idx.shape = (8,)\n",
      "temporal_idx.shape = (8,)\n",
      "idx.shape = (8, 2)\n",
      "slices.shape = (8, 32)\n",
      "slices.shape = (8, 1, 32)\n",
      "len(layer_ta) = 11\n",
      "===========================================================================\n",
      "dilation = 2\n",
      "conv_input.shape = (8, 100, 32)\n",
      "batch_idx.shape = (16,)\n",
      "temporal_idx.shape = (16,)\n",
      "idx.shape = (16, 2)\n",
      "slices.shape = (16, 32)\n",
      "slices.shape = (8, 2, 32)\n",
      "len(layer_ta) = 12\n",
      "===========================================================================\n",
      "dilation = 4\n",
      "conv_input.shape = (8, 100, 32)\n",
      "batch_idx.shape = (32,)\n",
      "temporal_idx.shape = (32,)\n",
      "idx.shape = (32, 2)\n",
      "slices.shape = (32, 32)\n",
      "slices.shape = (8, 4, 32)\n",
      "len(layer_ta) = 14\n",
      "===========================================================================\n",
      "dilation = 8\n",
      "conv_input.shape = (8, 100, 32)\n",
      "batch_idx.shape = (64,)\n",
      "temporal_idx.shape = (64,)\n",
      "idx.shape = (64, 2)\n",
      "slices.shape = (64, 32)\n",
      "slices.shape = (8, 8, 32)\n",
      "len(layer_ta) = 18\n",
      "===========================================================================\n",
      "dilation = 16\n",
      "conv_input.shape = (8, 100, 32)\n",
      "batch_idx.shape = (128,)\n",
      "temporal_idx.shape = (128,)\n",
      "idx.shape = (128, 2)\n",
      "slices.shape = (128, 32)\n",
      "slices.shape = (8, 16, 32)\n",
      "len(layer_ta) = 26\n",
      "===========================================================================\n",
      "dilation = 32\n",
      "conv_input.shape = (8, 100, 32)\n",
      "batch_idx.shape = (256,)\n",
      "temporal_idx.shape = (256,)\n",
      "idx.shape = (256, 2)\n",
      "slices.shape = (256, 32)\n",
      "slices.shape = (8, 32, 32)\n",
      "len(layer_ta) = 42\n",
      "===========================================================================\n",
      "dilation = 64\n",
      "conv_input.shape = (8, 100, 32)\n",
      "batch_idx.shape = (512,)\n",
      "temporal_idx.shape = (512,)\n",
      "idx.shape = (512, 2)\n",
      "slices.shape = (512, 32)\n",
      "slices.shape = (8, 64, 32)\n",
      "len(layer_ta) = 74\n",
      "===========================================================================\n",
      "dilation = 128\n",
      "conv_input.shape = (8, 100, 32)\n",
      "batch_idx.shape = (1024,)\n",
      "temporal_idx.shape = (1024,)\n",
      "idx.shape = (1024, 2)\n",
      "slices.shape = (1024, 32)\n",
      "slices.shape = (8, 128, 32)\n",
      "len(layer_ta) = 138\n"
     ]
    }
   ],
   "source": [
    "def print_shape(xs, name):\n",
    "    print \"{0}.shape = {1}\".format(name, xs.shape)\n",
    "\n",
    "# initialize state tensor arraysj\n",
    "state_queues = []\n",
    "for i, (conv_input, dilation) in enumerate(zip(conv_inputs, self.dilations)):\n",
    "    \n",
    "    print 75*\"=\"\n",
    "    print \"dilation = {0}\".format(dilation)\n",
    "    print_shape(conv_input, \"conv_input\")\n",
    "    \n",
    "    batch_idx = tf.range(batch_size)\n",
    "    batch_idx = tf.tile(tf.expand_dims(batch_idx, 1), (1, dilation))\n",
    "    batch_idx = tf.reshape(batch_idx, [-1])\n",
    "    print_shape(batch_idx, \"batch_idx\")\n",
    "    \n",
    "    queue_begin_time = self.encode_len - dilation - 1\n",
    "    temporal_idx = tf.expand_dims(queue_begin_time, 1) + tf.expand_dims(tf.range(dilation), 0)\n",
    "    temporal_idx = tf.reshape(temporal_idx, [-1])\n",
    "    print_shape(temporal_idx, \"temporal_idx\")\n",
    "\n",
    "    idx = tf.stack([batch_idx, temporal_idx], axis=1)\n",
    "    print_shape(idx, \"idx\")\n",
    "    slices = tf.gather_nd(conv_input, idx)\n",
    "    print_shape(slices, \"slices\")\n",
    "    slices = tf.reshape(slices, (batch_size, dilation, shape(conv_input, 2)))\n",
    "    print_shape(slices, \"slices\")\n",
    "    \n",
    "    print \"len(layer_ta) = {0}\".format(dilation + self.num_decode_steps)\n",
    "    layer_ta = tf.TensorArray(dtype=tf.float32, size=dilation + self.num_decode_steps)\n",
    "    layer_ta = layer_ta.unstack(tf.transpose(slices, (1, 0, 2)))\n",
    "    state_queues.append(layer_ta)\n",
    "\n",
    "# initialize feature tensor array\n",
    "features_ta = tf.TensorArray(dtype=tf.float32, size=self.num_decode_steps)\n",
    "features_ta = features_ta.unstack(tf.transpose(features, (1, 0, 2)))\n",
    "\n",
    "# initialize output tensor array\n",
    "emit_ta = tf.TensorArray(size=self.num_decode_steps, dtype=tf.float32)\n",
    "\n",
    "# initialize other loop vars\n",
    "elements_finished = 0 >= self.decode_len\n",
    "time = tf.constant(0, dtype=tf.int32)\n",
    "\n",
    "# get initial x input\n",
    "current_idx = tf.stack([tf.range(tf.shape(self.encode_len)[0]), self.encode_len - 1], axis=1)\n",
    "initial_input = tf.gather_nd(x, current_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(8), Dimension(32)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_ta.read(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_outputs, updated_queues = [], []\n",
    "i = 0 \n",
    "time = 0\n",
    "current_input = initial_input\n",
    "queues = state_queues\n",
    "conv_input, queue, dilation = zip(conv_inputs, queues, self.dilations)[3]\n",
    "\n",
    "current_features = features_ta.read(time)\n",
    "current_input = tf.concat([current_input, current_features], axis=1)\n",
    "\n",
    "with tf.variable_scope('x-proj-decode', reuse=True):\n",
    "    w_x_proj = tf.get_variable('weights')\n",
    "    b_x_proj = tf.get_variable('biases')\n",
    "    x_proj = tf.nn.tanh(tf.matmul(current_input, w_x_proj) + b_x_proj)\n",
    "\n",
    "state = queue.read(time)\n",
    "with tf.variable_scope('dilated-conv-decode-{}'.format(i), reuse=True):\n",
    "    w_conv = tf.get_variable('weights'.format(i))\n",
    "    b_conv = tf.get_variable('biases'.format(i))\n",
    "    dilated_conv = tf.matmul(state, w_conv[0, :, :]) + tf.matmul(x_proj, w_conv[1, :, :]) + b_conv\n",
    "conv_filter, conv_gate = tf.split(dilated_conv, 2, axis=1)\n",
    "dilated_conv = tf.nn.tanh(conv_filter)*tf.nn.sigmoid(conv_gate)\n",
    "\n",
    "with tf.variable_scope('dilated-conv-proj-decode-{}'.format(i), reuse=True):\n",
    "    w_proj = tf.get_variable('weights'.format(i))\n",
    "    b_proj = tf.get_variable('biases'.format(i))\n",
    "    concat_outputs = tf.matmul(dilated_conv, w_proj) + b_proj\n",
    "skips, residuals = tf.split(concat_outputs, [self.skip_channels, self.residual_channels], axis=1)\n",
    "\n",
    "x_proj += residuals\n",
    "skip_outputs.append(skips)\n",
    "updated_queues.append(queue.write(time + dilation, x_proj))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "rchan = 1\n",
    "seq_len = 30\n",
    "dilation = 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>enc_out[0,0]</td>\n",
       "      <td>enc_out[0,1]</td>\n",
       "      <td>enc_out[0,2]</td>\n",
       "      <td>enc_out[0,3]</td>\n",
       "      <td>enc_out[0,4]</td>\n",
       "      <td>enc_out[0,5]</td>\n",
       "      <td>enc_out[0,6]</td>\n",
       "      <td>enc_out[0,7]</td>\n",
       "      <td>enc_out[0,8]</td>\n",
       "      <td>enc_out[0,9]</td>\n",
       "      <td>enc_out[0,10]</td>\n",
       "      <td>enc_out[0,11]</td>\n",
       "      <td>enc_out[0,12]</td>\n",
       "      <td>enc_out[0,13]</td>\n",
       "      <td>enc_out[0,14]</td>\n",
       "      <td>enc_out[0,15]</td>\n",
       "      <td>enc_out[0,16]</td>\n",
       "      <td>enc_out[0,17]</td>\n",
       "      <td>enc_out[0,18]</td>\n",
       "      <td>enc_out[0,19]</td>\n",
       "      <td>enc_out[0,20]</td>\n",
       "      <td>enc_out[0,21]</td>\n",
       "      <td>enc_out[0,22]</td>\n",
       "      <td>enc_out[0,23]</td>\n",
       "      <td>enc_out[0,24]</td>\n",
       "      <td>enc_out[0,25]</td>\n",
       "      <td>enc_out[0,26]</td>\n",
       "      <td>enc_out[0,27]</td>\n",
       "      <td>enc_out[0,28]</td>\n",
       "      <td>enc_out[0,29]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>enc_out[1,0]</td>\n",
       "      <td>enc_out[1,1]</td>\n",
       "      <td>enc_out[1,2]</td>\n",
       "      <td>enc_out[1,3]</td>\n",
       "      <td>enc_out[1,4]</td>\n",
       "      <td>enc_out[1,5]</td>\n",
       "      <td>enc_out[1,6]</td>\n",
       "      <td>enc_out[1,7]</td>\n",
       "      <td>enc_out[1,8]</td>\n",
       "      <td>enc_out[1,9]</td>\n",
       "      <td>enc_out[1,10]</td>\n",
       "      <td>enc_out[1,11]</td>\n",
       "      <td>enc_out[1,12]</td>\n",
       "      <td>enc_out[1,13]</td>\n",
       "      <td>enc_out[1,14]</td>\n",
       "      <td>enc_out[1,15]</td>\n",
       "      <td>enc_out[1,16]</td>\n",
       "      <td>enc_out[1,17]</td>\n",
       "      <td>enc_out[1,18]</td>\n",
       "      <td>enc_out[1,19]</td>\n",
       "      <td>enc_out[1,20]</td>\n",
       "      <td>enc_out[1,21]</td>\n",
       "      <td>enc_out[1,22]</td>\n",
       "      <td>enc_out[1,23]</td>\n",
       "      <td>enc_out[1,24]</td>\n",
       "      <td>enc_out[1,25]</td>\n",
       "      <td>enc_out[1,26]</td>\n",
       "      <td>enc_out[1,27]</td>\n",
       "      <td>enc_out[1,28]</td>\n",
       "      <td>enc_out[1,29]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>enc_out[2,0]</td>\n",
       "      <td>enc_out[2,1]</td>\n",
       "      <td>enc_out[2,2]</td>\n",
       "      <td>enc_out[2,3]</td>\n",
       "      <td>enc_out[2,4]</td>\n",
       "      <td>enc_out[2,5]</td>\n",
       "      <td>enc_out[2,6]</td>\n",
       "      <td>enc_out[2,7]</td>\n",
       "      <td>enc_out[2,8]</td>\n",
       "      <td>enc_out[2,9]</td>\n",
       "      <td>enc_out[2,10]</td>\n",
       "      <td>enc_out[2,11]</td>\n",
       "      <td>enc_out[2,12]</td>\n",
       "      <td>enc_out[2,13]</td>\n",
       "      <td>enc_out[2,14]</td>\n",
       "      <td>enc_out[2,15]</td>\n",
       "      <td>enc_out[2,16]</td>\n",
       "      <td>enc_out[2,17]</td>\n",
       "      <td>enc_out[2,18]</td>\n",
       "      <td>enc_out[2,19]</td>\n",
       "      <td>enc_out[2,20]</td>\n",
       "      <td>enc_out[2,21]</td>\n",
       "      <td>enc_out[2,22]</td>\n",
       "      <td>enc_out[2,23]</td>\n",
       "      <td>enc_out[2,24]</td>\n",
       "      <td>enc_out[2,25]</td>\n",
       "      <td>enc_out[2,26]</td>\n",
       "      <td>enc_out[2,27]</td>\n",
       "      <td>enc_out[2,28]</td>\n",
       "      <td>enc_out[2,29]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enc_out[3,0]</td>\n",
       "      <td>enc_out[3,1]</td>\n",
       "      <td>enc_out[3,2]</td>\n",
       "      <td>enc_out[3,3]</td>\n",
       "      <td>enc_out[3,4]</td>\n",
       "      <td>enc_out[3,5]</td>\n",
       "      <td>enc_out[3,6]</td>\n",
       "      <td>enc_out[3,7]</td>\n",
       "      <td>enc_out[3,8]</td>\n",
       "      <td>enc_out[3,9]</td>\n",
       "      <td>enc_out[3,10]</td>\n",
       "      <td>enc_out[3,11]</td>\n",
       "      <td>enc_out[3,12]</td>\n",
       "      <td>enc_out[3,13]</td>\n",
       "      <td>enc_out[3,14]</td>\n",
       "      <td>enc_out[3,15]</td>\n",
       "      <td>enc_out[3,16]</td>\n",
       "      <td>enc_out[3,17]</td>\n",
       "      <td>enc_out[3,18]</td>\n",
       "      <td>enc_out[3,19]</td>\n",
       "      <td>enc_out[3,20]</td>\n",
       "      <td>enc_out[3,21]</td>\n",
       "      <td>enc_out[3,22]</td>\n",
       "      <td>enc_out[3,23]</td>\n",
       "      <td>enc_out[3,24]</td>\n",
       "      <td>enc_out[3,25]</td>\n",
       "      <td>enc_out[3,26]</td>\n",
       "      <td>enc_out[3,27]</td>\n",
       "      <td>enc_out[3,28]</td>\n",
       "      <td>enc_out[3,29]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>enc_out[4,0]</td>\n",
       "      <td>enc_out[4,1]</td>\n",
       "      <td>enc_out[4,2]</td>\n",
       "      <td>enc_out[4,3]</td>\n",
       "      <td>enc_out[4,4]</td>\n",
       "      <td>enc_out[4,5]</td>\n",
       "      <td>enc_out[4,6]</td>\n",
       "      <td>enc_out[4,7]</td>\n",
       "      <td>enc_out[4,8]</td>\n",
       "      <td>enc_out[4,9]</td>\n",
       "      <td>enc_out[4,10]</td>\n",
       "      <td>enc_out[4,11]</td>\n",
       "      <td>enc_out[4,12]</td>\n",
       "      <td>enc_out[4,13]</td>\n",
       "      <td>enc_out[4,14]</td>\n",
       "      <td>enc_out[4,15]</td>\n",
       "      <td>enc_out[4,16]</td>\n",
       "      <td>enc_out[4,17]</td>\n",
       "      <td>enc_out[4,18]</td>\n",
       "      <td>enc_out[4,19]</td>\n",
       "      <td>enc_out[4,20]</td>\n",
       "      <td>enc_out[4,21]</td>\n",
       "      <td>enc_out[4,22]</td>\n",
       "      <td>enc_out[4,23]</td>\n",
       "      <td>enc_out[4,24]</td>\n",
       "      <td>enc_out[4,25]</td>\n",
       "      <td>enc_out[4,26]</td>\n",
       "      <td>enc_out[4,27]</td>\n",
       "      <td>enc_out[4,28]</td>\n",
       "      <td>enc_out[4,29]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>enc_out[5,0]</td>\n",
       "      <td>enc_out[5,1]</td>\n",
       "      <td>enc_out[5,2]</td>\n",
       "      <td>enc_out[5,3]</td>\n",
       "      <td>enc_out[5,4]</td>\n",
       "      <td>enc_out[5,5]</td>\n",
       "      <td>enc_out[5,6]</td>\n",
       "      <td>enc_out[5,7]</td>\n",
       "      <td>enc_out[5,8]</td>\n",
       "      <td>enc_out[5,9]</td>\n",
       "      <td>enc_out[5,10]</td>\n",
       "      <td>enc_out[5,11]</td>\n",
       "      <td>enc_out[5,12]</td>\n",
       "      <td>enc_out[5,13]</td>\n",
       "      <td>enc_out[5,14]</td>\n",
       "      <td>enc_out[5,15]</td>\n",
       "      <td>enc_out[5,16]</td>\n",
       "      <td>enc_out[5,17]</td>\n",
       "      <td>enc_out[5,18]</td>\n",
       "      <td>enc_out[5,19]</td>\n",
       "      <td>enc_out[5,20]</td>\n",
       "      <td>enc_out[5,21]</td>\n",
       "      <td>enc_out[5,22]</td>\n",
       "      <td>enc_out[5,23]</td>\n",
       "      <td>enc_out[5,24]</td>\n",
       "      <td>enc_out[5,25]</td>\n",
       "      <td>enc_out[5,26]</td>\n",
       "      <td>enc_out[5,27]</td>\n",
       "      <td>enc_out[5,28]</td>\n",
       "      <td>enc_out[5,29]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>enc_out[6,0]</td>\n",
       "      <td>enc_out[6,1]</td>\n",
       "      <td>enc_out[6,2]</td>\n",
       "      <td>enc_out[6,3]</td>\n",
       "      <td>enc_out[6,4]</td>\n",
       "      <td>enc_out[6,5]</td>\n",
       "      <td>enc_out[6,6]</td>\n",
       "      <td>enc_out[6,7]</td>\n",
       "      <td>enc_out[6,8]</td>\n",
       "      <td>enc_out[6,9]</td>\n",
       "      <td>enc_out[6,10]</td>\n",
       "      <td>enc_out[6,11]</td>\n",
       "      <td>enc_out[6,12]</td>\n",
       "      <td>enc_out[6,13]</td>\n",
       "      <td>enc_out[6,14]</td>\n",
       "      <td>enc_out[6,15]</td>\n",
       "      <td>enc_out[6,16]</td>\n",
       "      <td>enc_out[6,17]</td>\n",
       "      <td>enc_out[6,18]</td>\n",
       "      <td>enc_out[6,19]</td>\n",
       "      <td>enc_out[6,20]</td>\n",
       "      <td>enc_out[6,21]</td>\n",
       "      <td>enc_out[6,22]</td>\n",
       "      <td>enc_out[6,23]</td>\n",
       "      <td>enc_out[6,24]</td>\n",
       "      <td>enc_out[6,25]</td>\n",
       "      <td>enc_out[6,26]</td>\n",
       "      <td>enc_out[6,27]</td>\n",
       "      <td>enc_out[6,28]</td>\n",
       "      <td>enc_out[6,29]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>enc_out[7,0]</td>\n",
       "      <td>enc_out[7,1]</td>\n",
       "      <td>enc_out[7,2]</td>\n",
       "      <td>enc_out[7,3]</td>\n",
       "      <td>enc_out[7,4]</td>\n",
       "      <td>enc_out[7,5]</td>\n",
       "      <td>enc_out[7,6]</td>\n",
       "      <td>enc_out[7,7]</td>\n",
       "      <td>enc_out[7,8]</td>\n",
       "      <td>enc_out[7,9]</td>\n",
       "      <td>enc_out[7,10]</td>\n",
       "      <td>enc_out[7,11]</td>\n",
       "      <td>enc_out[7,12]</td>\n",
       "      <td>enc_out[7,13]</td>\n",
       "      <td>enc_out[7,14]</td>\n",
       "      <td>enc_out[7,15]</td>\n",
       "      <td>enc_out[7,16]</td>\n",
       "      <td>enc_out[7,17]</td>\n",
       "      <td>enc_out[7,18]</td>\n",
       "      <td>enc_out[7,19]</td>\n",
       "      <td>enc_out[7,20]</td>\n",
       "      <td>enc_out[7,21]</td>\n",
       "      <td>enc_out[7,22]</td>\n",
       "      <td>enc_out[7,23]</td>\n",
       "      <td>enc_out[7,24]</td>\n",
       "      <td>enc_out[7,25]</td>\n",
       "      <td>enc_out[7,26]</td>\n",
       "      <td>enc_out[7,27]</td>\n",
       "      <td>enc_out[7,28]</td>\n",
       "      <td>enc_out[7,29]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0             1             2             3             4   \\\n",
       "0  enc_out[0,0]  enc_out[0,1]  enc_out[0,2]  enc_out[0,3]  enc_out[0,4]   \n",
       "1  enc_out[1,0]  enc_out[1,1]  enc_out[1,2]  enc_out[1,3]  enc_out[1,4]   \n",
       "2  enc_out[2,0]  enc_out[2,1]  enc_out[2,2]  enc_out[2,3]  enc_out[2,4]   \n",
       "3  enc_out[3,0]  enc_out[3,1]  enc_out[3,2]  enc_out[3,3]  enc_out[3,4]   \n",
       "4  enc_out[4,0]  enc_out[4,1]  enc_out[4,2]  enc_out[4,3]  enc_out[4,4]   \n",
       "5  enc_out[5,0]  enc_out[5,1]  enc_out[5,2]  enc_out[5,3]  enc_out[5,4]   \n",
       "6  enc_out[6,0]  enc_out[6,1]  enc_out[6,2]  enc_out[6,3]  enc_out[6,4]   \n",
       "7  enc_out[7,0]  enc_out[7,1]  enc_out[7,2]  enc_out[7,3]  enc_out[7,4]   \n",
       "\n",
       "             5             6             7             8             9   \\\n",
       "0  enc_out[0,5]  enc_out[0,6]  enc_out[0,7]  enc_out[0,8]  enc_out[0,9]   \n",
       "1  enc_out[1,5]  enc_out[1,6]  enc_out[1,7]  enc_out[1,8]  enc_out[1,9]   \n",
       "2  enc_out[2,5]  enc_out[2,6]  enc_out[2,7]  enc_out[2,8]  enc_out[2,9]   \n",
       "3  enc_out[3,5]  enc_out[3,6]  enc_out[3,7]  enc_out[3,8]  enc_out[3,9]   \n",
       "4  enc_out[4,5]  enc_out[4,6]  enc_out[4,7]  enc_out[4,8]  enc_out[4,9]   \n",
       "5  enc_out[5,5]  enc_out[5,6]  enc_out[5,7]  enc_out[5,8]  enc_out[5,9]   \n",
       "6  enc_out[6,5]  enc_out[6,6]  enc_out[6,7]  enc_out[6,8]  enc_out[6,9]   \n",
       "7  enc_out[7,5]  enc_out[7,6]  enc_out[7,7]  enc_out[7,8]  enc_out[7,9]   \n",
       "\n",
       "              10             11             12             13             14  \\\n",
       "0  enc_out[0,10]  enc_out[0,11]  enc_out[0,12]  enc_out[0,13]  enc_out[0,14]   \n",
       "1  enc_out[1,10]  enc_out[1,11]  enc_out[1,12]  enc_out[1,13]  enc_out[1,14]   \n",
       "2  enc_out[2,10]  enc_out[2,11]  enc_out[2,12]  enc_out[2,13]  enc_out[2,14]   \n",
       "3  enc_out[3,10]  enc_out[3,11]  enc_out[3,12]  enc_out[3,13]  enc_out[3,14]   \n",
       "4  enc_out[4,10]  enc_out[4,11]  enc_out[4,12]  enc_out[4,13]  enc_out[4,14]   \n",
       "5  enc_out[5,10]  enc_out[5,11]  enc_out[5,12]  enc_out[5,13]  enc_out[5,14]   \n",
       "6  enc_out[6,10]  enc_out[6,11]  enc_out[6,12]  enc_out[6,13]  enc_out[6,14]   \n",
       "7  enc_out[7,10]  enc_out[7,11]  enc_out[7,12]  enc_out[7,13]  enc_out[7,14]   \n",
       "\n",
       "              15             16             17             18             19  \\\n",
       "0  enc_out[0,15]  enc_out[0,16]  enc_out[0,17]  enc_out[0,18]  enc_out[0,19]   \n",
       "1  enc_out[1,15]  enc_out[1,16]  enc_out[1,17]  enc_out[1,18]  enc_out[1,19]   \n",
       "2  enc_out[2,15]  enc_out[2,16]  enc_out[2,17]  enc_out[2,18]  enc_out[2,19]   \n",
       "3  enc_out[3,15]  enc_out[3,16]  enc_out[3,17]  enc_out[3,18]  enc_out[3,19]   \n",
       "4  enc_out[4,15]  enc_out[4,16]  enc_out[4,17]  enc_out[4,18]  enc_out[4,19]   \n",
       "5  enc_out[5,15]  enc_out[5,16]  enc_out[5,17]  enc_out[5,18]  enc_out[5,19]   \n",
       "6  enc_out[6,15]  enc_out[6,16]  enc_out[6,17]  enc_out[6,18]  enc_out[6,19]   \n",
       "7  enc_out[7,15]  enc_out[7,16]  enc_out[7,17]  enc_out[7,18]  enc_out[7,19]   \n",
       "\n",
       "              20             21             22             23             24  \\\n",
       "0  enc_out[0,20]  enc_out[0,21]  enc_out[0,22]  enc_out[0,23]  enc_out[0,24]   \n",
       "1  enc_out[1,20]  enc_out[1,21]  enc_out[1,22]  enc_out[1,23]  enc_out[1,24]   \n",
       "2  enc_out[2,20]  enc_out[2,21]  enc_out[2,22]  enc_out[2,23]  enc_out[2,24]   \n",
       "3  enc_out[3,20]  enc_out[3,21]  enc_out[3,22]  enc_out[3,23]  enc_out[3,24]   \n",
       "4  enc_out[4,20]  enc_out[4,21]  enc_out[4,22]  enc_out[4,23]  enc_out[4,24]   \n",
       "5  enc_out[5,20]  enc_out[5,21]  enc_out[5,22]  enc_out[5,23]  enc_out[5,24]   \n",
       "6  enc_out[6,20]  enc_out[6,21]  enc_out[6,22]  enc_out[6,23]  enc_out[6,24]   \n",
       "7  enc_out[7,20]  enc_out[7,21]  enc_out[7,22]  enc_out[7,23]  enc_out[7,24]   \n",
       "\n",
       "              25             26             27             28             29  \n",
       "0  enc_out[0,25]  enc_out[0,26]  enc_out[0,27]  enc_out[0,28]  enc_out[0,29]  \n",
       "1  enc_out[1,25]  enc_out[1,26]  enc_out[1,27]  enc_out[1,28]  enc_out[1,29]  \n",
       "2  enc_out[2,25]  enc_out[2,26]  enc_out[2,27]  enc_out[2,28]  enc_out[2,29]  \n",
       "3  enc_out[3,25]  enc_out[3,26]  enc_out[3,27]  enc_out[3,28]  enc_out[3,29]  \n",
       "4  enc_out[4,25]  enc_out[4,26]  enc_out[4,27]  enc_out[4,28]  enc_out[4,29]  \n",
       "5  enc_out[5,25]  enc_out[5,26]  enc_out[5,27]  enc_out[5,28]  enc_out[5,29]  \n",
       "6  enc_out[6,25]  enc_out[6,26]  enc_out[6,27]  enc_out[6,28]  enc_out[6,29]  \n",
       "7  enc_out[7,25]  enc_out[7,26]  enc_out[7,27]  enc_out[7,28]  enc_out[7,29]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con_in = np.arange(batch_size*seq_len*rchan).reshape(batch_size, seq_len, rchan)\n",
    "con_in = np.array([[\"enc_out[{0},{1}]\".format(i, j) for j in xrange(seq_len)] for i in xrange(batch_size)])\n",
    "pd.DataFrame(con_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi = np.arange(batch_size)\n",
    "bi = np.tile(bi.reshape(-1, 1), (1, dilation))\n",
    "bi = bi.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "el = 10 + np.arange(batch_size)\n",
    "qbt = el - dilation - 1\n",
    "ti = qbt.reshape(-1, 1) + np.arange(dilation)\n",
    "ti = ti.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.c_[bi, ti]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = np.array([con_in[i[0], i[1]] for i in idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = slices.reshape(batch_size, dilation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['enc_out[0,5]', 'enc_out[0,6]', 'enc_out[0,7]', 'enc_out[0,8]'],\n",
       "       ['enc_out[1,6]', 'enc_out[1,7]', 'enc_out[1,8]', 'enc_out[1,9]'],\n",
       "       ['enc_out[2,7]', 'enc_out[2,8]', 'enc_out[2,9]', 'enc_out[2,10]'],\n",
       "       ['enc_out[3,8]', 'enc_out[3,9]', 'enc_out[3,10]', 'enc_out[3,11]'],\n",
       "       ['enc_out[4,9]', 'enc_out[4,10]', 'enc_out[4,11]',\n",
       "        'enc_out[4,12]'],\n",
       "       ['enc_out[5,10]', 'enc_out[5,11]', 'enc_out[5,12]',\n",
       "        'enc_out[5,13]'],\n",
       "       ['enc_out[6,11]', 'enc_out[6,12]', 'enc_out[6,13]',\n",
       "        'enc_out[6,14]'],\n",
       "       ['enc_out[7,12]', 'enc_out[7,13]', 'enc_out[7,14]',\n",
       "        'enc_out[7,15]']], dtype='|S13')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.take(np.arange(9).reshape(3, 3), [0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2],\n",
       "       [3, 4, 5],\n",
       "       [6, 7, 8]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.arange(9).reshape(3, 3)\n",
    "A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[(1, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
