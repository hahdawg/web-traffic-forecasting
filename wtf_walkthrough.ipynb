{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_convolution_layer(inputs, output_units, convolution_width, causal=False, dilation_rate=[1], bias=True,\n",
    "                               activation=None, dropout=None, scope='temporal-convolution-layer', reuse=False):\n",
    "    \"\"\"\n",
    "    Convolution over the temporal axis of sequence data.\n",
    "    Args:\n",
    "        inputs: Tensor of shape [batch size, max sequence length, input_units].\n",
    "        output_units: Output channels for convolution.\n",
    "        convolution_width: Number of timesteps to use in convolution.\n",
    "        causal: Output at timestep t is a function of inputs at or before timestep t.\n",
    "        dilation_rate:  Dilation rate along temporal axis.\n",
    "    Returns:\n",
    "        Tensor of shape [batch size, max sequence length, output_units].\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        if causal:\n",
    "            shift = int(convolution_width / 2) + int(int(dilation_rate[0] - 1) / 2)\n",
    "            pad = tf.zeros([tf.shape(inputs)[0], shift, inputs.shape.as_list()[2]])\n",
    "            inputs = tf.concat([pad, inputs], axis=1)\n",
    "\n",
    "        W = tf.get_variable(\n",
    "            name='weights',\n",
    "            initializer=tf.random_normal_initializer(\n",
    "                mean=0,\n",
    "                stddev=1.0 / tf.sqrt(float(convolution_width)*float(shape(inputs, 2)))\n",
    "            ),\n",
    "            shape=[convolution_width, shape(inputs, 2), output_units]\n",
    "        )\n",
    "\n",
    "        z = tf.nn.convolution(inputs, W, padding='SAME', dilation_rate=dilation_rate)\n",
    "        if bias:\n",
    "            b = tf.get_variable(\n",
    "                name='biases',\n",
    "                initializer=tf.constant_initializer(),\n",
    "                shape=[output_units]\n",
    "            )\n",
    "            z = z + b\n",
    "        z = activation(z) if activation else z\n",
    "        z = tf.nn.dropout(z, dropout) if dropout is not None else z\n",
    "        z = z[:, :-shift, :] if causal else z\n",
    "        return z\n",
    "\n",
    "\n",
    "def time_distributed_dense_layer(inputs, output_units, bias=True, activation=None, batch_norm=None,\n",
    "                                 dropout=None, scope='time-distributed-dense-layer', reuse=False):\n",
    "    \"\"\"\n",
    "    Applies a shared dense layer to each timestep of a tensor of shape [batch_size, max_seq_len, input_units]\n",
    "    to produce a tensor of shape [batch_size, max_seq_len, output_units].\n",
    "    Args:\n",
    "        inputs: Tensor of shape [batch size, max sequence length, ...].\n",
    "        output_units: Number of output units.\n",
    "        activation: activation function.\n",
    "        dropout: dropout keep prob.\n",
    "    Returns:\n",
    "        Tensor of shape [batch size, max sequence length, output_units].\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        W = tf.get_variable(\n",
    "            name='weights',\n",
    "            initializer=tf.random_normal_initializer(mean=0.0, stddev=1.0 / float(shape(inputs, -1))),\n",
    "            shape=[shape(inputs, -1), output_units]\n",
    "        )\n",
    "        z = tf.einsum('ijk,kl->ijl', inputs, W)\n",
    "        if bias:\n",
    "            b = tf.get_variable(\n",
    "                name='biases',\n",
    "                initializer=tf.constant_initializer(),\n",
    "                shape=[output_units]\n",
    "            )\n",
    "            z = z + b\n",
    "\n",
    "        if batch_norm is not None:\n",
    "            z = tf.layers.batch_normalization(z, training=batch_norm, reuse=reuse)\n",
    "\n",
    "        z = activation(z) if activation else z\n",
    "        z = tf.nn.dropout(z, dropout) if dropout is not None else z\n",
    "        return z\n",
    "\n",
    "\n",
    "def shape(tensor, dim=None):\n",
    "    \"\"\"Get tensor shape/dimension as list/int\"\"\"\n",
    "    if dim is None:\n",
    "        return tensor.shape.as_list()\n",
    "    else:\n",
    "        return tensor.shape.as_list()[dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(x, features):\n",
    "    x = tf.concat([x, features], axis=2)\n",
    "\n",
    "    inputs = time_distributed_dense_layer(\n",
    "        inputs=x,\n",
    "        output_units=self.residual_channels,\n",
    "        activation=tf.nn.tanh,\n",
    "        scope='x-proj-encode'\n",
    "    )\n",
    "\n",
    "    skip_outputs = []\n",
    "    conv_inputs = [inputs]\n",
    "    for i, (dilation, filter_width) in enumerate(zip(self.dilations, self.filter_widths)):\n",
    "        dilated_conv = temporal_convolution_layer(\n",
    "            inputs=inputs,\n",
    "            output_units=2*self.residual_channels,\n",
    "            convolution_width=filter_width,\n",
    "            causal=True,\n",
    "            dilation_rate=[dilation],\n",
    "            scope='dilated-conv-encode-{}'.format(i)\n",
    "        )\n",
    "        conv_filter, conv_gate = tf.split(dilated_conv, 2, axis=2)\n",
    "        dilated_conv = tf.nn.tanh(conv_filter)*tf.nn.sigmoid(conv_gate)\n",
    "\n",
    "        outputs = time_distributed_dense_layer(\n",
    "            inputs=dilated_conv,\n",
    "            output_units=self.skip_channels + self.residual_channels,\n",
    "            scope='dilated-conv-proj-encode-{}'.format(i)\n",
    "        )\n",
    "        skips, residuals = tf.split(outputs, [self.skip_channels, self.residual_channels], axis=2)\n",
    "\n",
    "        inputs += residuals\n",
    "        conv_inputs.append(inputs)\n",
    "        skip_outputs.append(skips)\n",
    "\n",
    "    skip_outputs = tf.nn.relu(tf.concat(skip_outputs, axis=2))\n",
    "    h = time_distributed_dense_layer(skip_outputs, 128, scope='dense-encode-1', activation=tf.nn.relu)\n",
    "    y_hat = time_distributed_dense_layer(h, 1, scope='dense-encode-2')\n",
    "\n",
    "    return y_hat, conv_inputs[:-1]\n",
    "\n",
    "\n",
    "def initialize_decode_params(x, features):\n",
    "    x = tf.concat([x, features], axis=2)\n",
    "\n",
    "    inputs = time_distributed_dense_layer(\n",
    "        inputs=x,\n",
    "        output_units=self.residual_channels,\n",
    "        activation=tf.nn.tanh,\n",
    "        scope='x-proj-decode'\n",
    "    )\n",
    "\n",
    "    skip_outputs = []\n",
    "    conv_inputs = [inputs]\n",
    "    for i, (dilation, filter_width) in enumerate(zip(self.dilations, self.filter_widths)):\n",
    "        dilated_conv = temporal_convolution_layer(\n",
    "            inputs=inputs,\n",
    "            output_units=2*self.residual_channels,\n",
    "            convolution_width=filter_width,\n",
    "            causal=True,\n",
    "            dilation_rate=[dilation],\n",
    "            scope='dilated-conv-decode-{}'.format(i)\n",
    "        )\n",
    "        conv_filter, conv_gate = tf.split(dilated_conv, 2, axis=2)\n",
    "        dilated_conv = tf.nn.tanh(conv_filter)*tf.nn.sigmoid(conv_gate)\n",
    "\n",
    "        outputs = time_distributed_dense_layer(\n",
    "            inputs=dilated_conv,\n",
    "            output_units=self.skip_channels + self.residual_channels,\n",
    "            scope='dilated-conv-proj-decode-{}'.format(i)\n",
    "        )\n",
    "        skips, residuals = tf.split(outputs, [self.skip_channels, self.residual_channels], axis=2)\n",
    "\n",
    "        inputs += residuals\n",
    "        conv_inputs.append(inputs)\n",
    "        skip_outputs.append(skips)\n",
    "\n",
    "    skip_outputs = tf.nn.relu(tf.concat(skip_outputs, axis=2))\n",
    "    h = time_distributed_dense_layer(skip_outputs, 128, scope='dense-decode-1', activation=tf.nn.relu)\n",
    "    y_hat = time_distributed_dense_layer(h, 1, scope='dense-decode-2')\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HelperClass(object):\n",
    "    pass\n",
    "\n",
    "self = HelperClass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "batch_size = 8\n",
    "seq_len = 100\n",
    "self.residual_channels = 32\n",
    "self.skip_channels = 32\n",
    "self.num_decode_steps=10\n",
    "self.decode_len = 64\n",
    "num_features = 4\n",
    "self.dilations = [2**i for i in range(8)]*3\n",
    "self.filter_widths = [2 for i in range(8)]*3\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[batch_size, seq_len])\n",
    "x = tf.expand_dims(x, 2)\n",
    "self.encode_len = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "features = tf.placeholder(tf.float32, shape=[batch_size, seq_len, num_features])\n",
    "\n",
    "y_hat_encode, conv_inputs = encode(x, features)\n",
    "initialize_decode_params(x, features);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 100, 1)\n",
      "24\n",
      "(8, 100, 32)\n"
     ]
    }
   ],
   "source": [
    "print y_hat_encode.shape\n",
    "print len(conv_inputs)\n",
    "print conv_inputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_shape(xs, name):\n",
    "    print \"{0}.shape = {1}\".format(name, xs.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['c000', 'c001', 'c002'],\n",
       "       ['c010', 'c011', 'c012'],\n",
       "       ['c020', 'c021', 'c022'],\n",
       "       ['c030', 'c031', 'c032'],\n",
       "       ['c040', 'c041', 'c042'],\n",
       "       ['c050', 'c051', 'c052'],\n",
       "       ['c060', 'c061', 'c062'],\n",
       "       ['c070', 'c071', 'c072'],\n",
       "       ['c080', 'c081', 'c082'],\n",
       "       ['c090', 'c091', 'c092'],\n",
       "       ['c0100', 'c0101', 'c0102'],\n",
       "       ['c0110', 'c0111', 'c0112'],\n",
       "       ['c0120', 'c0121', 'c0122'],\n",
       "       ['c0130', 'c0131', 'c0132'],\n",
       "       ['c0140', 'c0141', 'c0142'],\n",
       "       ['c0150', 'c0151', 'c0152'],\n",
       "       ['c0160', 'c0161', 'c0162'],\n",
       "       ['c0170', 'c0171', 'c0172'],\n",
       "       ['c0180', 'c0181', 'c0182'],\n",
       "       ['c0190', 'c0191', 'c0192'],\n",
       "       ['c0200', 'c0201', 'c0202'],\n",
       "       ['c0210', 'c0211', 'c0212'],\n",
       "       ['c0220', 'c0221', 'c0222'],\n",
       "       ['c0230', 'c0231', 'c0232'],\n",
       "       ['c0240', 'c0241', 'c0242'],\n",
       "       ['c0250', 'c0251', 'c0252'],\n",
       "       ['c0260', 'c0261', 'c0262'],\n",
       "       ['c0270', 'c0271', 'c0272'],\n",
       "       ['c0280', 'c0281', 'c0282'],\n",
       "       ['c0290', 'c0291', 'c0292'],\n",
       "       ['c0300', 'c0301', 'c0302'],\n",
       "       ['c0310', 'c0311', 'c0312'],\n",
       "       ['c0320', 'c0321', 'c0322'],\n",
       "       ['c0330', 'c0331', 'c0332'],\n",
       "       ['c0340', 'c0341', 'c0342'],\n",
       "       ['c0350', 'c0351', 'c0352'],\n",
       "       ['c0360', 'c0361', 'c0362'],\n",
       "       ['c0370', 'c0371', 'c0372'],\n",
       "       ['c0380', 'c0381', 'c0382'],\n",
       "       ['c0390', 'c0391', 'c0392'],\n",
       "       ['c0400', 'c0401', 'c0402'],\n",
       "       ['c0410', 'c0411', 'c0412'],\n",
       "       ['c0420', 'c0421', 'c0422'],\n",
       "       ['c0430', 'c0431', 'c0432'],\n",
       "       ['c0440', 'c0441', 'c0442'],\n",
       "       ['c0450', 'c0451', 'c0452'],\n",
       "       ['c0460', 'c0461', 'c0462'],\n",
       "       ['c0470', 'c0471', 'c0472'],\n",
       "       ['c0480', 'c0481', 'c0482'],\n",
       "       ['c0490', 'c0491', 'c0492'],\n",
       "       ['c0500', 'c0501', 'c0502'],\n",
       "       ['c0510', 'c0511', 'c0512'],\n",
       "       ['c0520', 'c0521', 'c0522'],\n",
       "       ['c0530', 'c0531', 'c0532'],\n",
       "       ['c0540', 'c0541', 'c0542'],\n",
       "       ['c0550', 'c0551', 'c0552'],\n",
       "       ['c0560', 'c0561', 'c0562'],\n",
       "       ['c0570', 'c0571', 'c0572'],\n",
       "       ['c0580', 'c0581', 'c0582'],\n",
       "       ['c0590', 'c0591', 'c0592'],\n",
       "       ['c0600', 'c0601', 'c0602'],\n",
       "       ['c0610', 'c0611', 'c0612'],\n",
       "       ['c0620', 'c0621', 'c0622'],\n",
       "       ['c0630', 'c0631', 'c0632'],\n",
       "       ['c0640', 'c0641', 'c0642'],\n",
       "       ['c0650', 'c0651', 'c0652'],\n",
       "       ['c0660', 'c0661', 'c0662'],\n",
       "       ['c0670', 'c0671', 'c0672'],\n",
       "       ['c0680', 'c0681', 'c0682'],\n",
       "       ['c0690', 'c0691', 'c0692'],\n",
       "       ['c0700', 'c0701', 'c0702'],\n",
       "       ['c0710', 'c0711', 'c0712'],\n",
       "       ['c0720', 'c0721', 'c0722'],\n",
       "       ['c0730', 'c0731', 'c0732'],\n",
       "       ['c0740', 'c0741', 'c0742'],\n",
       "       ['c0750', 'c0751', 'c0752'],\n",
       "       ['c0760', 'c0761', 'c0762'],\n",
       "       ['c0770', 'c0771', 'c0772'],\n",
       "       ['c0780', 'c0781', 'c0782'],\n",
       "       ['c0790', 'c0791', 'c0792'],\n",
       "       ['c0800', 'c0801', 'c0802'],\n",
       "       ['c0810', 'c0811', 'c0812'],\n",
       "       ['c0820', 'c0821', 'c0822'],\n",
       "       ['c0830', 'c0831', 'c0832'],\n",
       "       ['c0840', 'c0841', 'c0842'],\n",
       "       ['c0850', 'c0851', 'c0852'],\n",
       "       ['c0860', 'c0861', 'c0862'],\n",
       "       ['c0870', 'c0871', 'c0872'],\n",
       "       ['c0880', 'c0881', 'c0882'],\n",
       "       ['c0890', 'c0891', 'c0892'],\n",
       "       ['c0900', 'c0901', 'c0902'],\n",
       "       ['c0910', 'c0911', 'c0912'],\n",
       "       ['c0920', 'c0921', 'c0922'],\n",
       "       ['c0930', 'c0931', 'c0932'],\n",
       "       ['c0940', 'c0941', 'c0942'],\n",
       "       ['c0950', 'c0951', 'c0952'],\n",
       "       ['c0960', 'c0961', 'c0962'],\n",
       "       ['c0970', 'c0971', 'c0972'],\n",
       "       ['c0980', 'c0981', 'c0982'],\n",
       "       ['c0990', 'c0991', 'c0992'],\n",
       "       ['c01000', 'c01001', 'c01002'],\n",
       "       ['c01010', 'c01011', 'c01012'],\n",
       "       ['c01020', 'c01021', 'c01022'],\n",
       "       ['c01030', 'c01031', 'c01032'],\n",
       "       ['c01040', 'c01041', 'c01042'],\n",
       "       ['c01050', 'c01051', 'c01052'],\n",
       "       ['c01060', 'c01061', 'c01062'],\n",
       "       ['c01070', 'c01071', 'c01072'],\n",
       "       ['c01080', 'c01081', 'c01082'],\n",
       "       ['c01090', 'c01091', 'c01092'],\n",
       "       ['c01100', 'c01101', 'c01102'],\n",
       "       ['c01110', 'c01111', 'c01112'],\n",
       "       ['c01120', 'c01121', 'c01122'],\n",
       "       ['c01130', 'c01131', 'c01132'],\n",
       "       ['c01140', 'c01141', 'c01142'],\n",
       "       ['c01150', 'c01151', 'c01152'],\n",
       "       ['c01160', 'c01161', 'c01162'],\n",
       "       ['c01170', 'c01171', 'c01172'],\n",
       "       ['c01180', 'c01181', 'c01182'],\n",
       "       ['c01190', 'c01191', 'c01192'],\n",
       "       ['c01200', 'c01201', 'c01202'],\n",
       "       ['c01210', 'c01211', 'c01212'],\n",
       "       ['c01220', 'c01221', 'c01222'],\n",
       "       ['c01230', 'c01231', 'c01232'],\n",
       "       ['c01240', 'c01241', 'c01242'],\n",
       "       ['c01250', 'c01251', 'c01252'],\n",
       "       ['c01260', 'c01261', 'c01262'],\n",
       "       ['c01270', 'c01271', 'c01272'],\n",
       "       ['c01280', 'c01281', 'c01282'],\n",
       "       ['c01290', 'c01291', 'c01292'],\n",
       "       ['c01300', 'c01301', 'c01302'],\n",
       "       ['c01310', 'c01311', 'c01312'],\n",
       "       ['c01320', 'c01321', 'c01322'],\n",
       "       ['c01330', 'c01331', 'c01332'],\n",
       "       ['c01340', 'c01341', 'c01342'],\n",
       "       ['c01350', 'c01351', 'c01352'],\n",
       "       ['c01360', 'c01361', 'c01362'],\n",
       "       ['c01370', 'c01371', 'c01372'],\n",
       "       ['c01380', 'c01381', 'c01382'],\n",
       "       ['c01390', 'c01391', 'c01392'],\n",
       "       ['c01400', 'c01401', 'c01402'],\n",
       "       ['c01410', 'c01411', 'c01412'],\n",
       "       ['c01420', 'c01421', 'c01422'],\n",
       "       ['c01430', 'c01431', 'c01432'],\n",
       "       ['c01440', 'c01441', 'c01442'],\n",
       "       ['c01450', 'c01451', 'c01452'],\n",
       "       ['c01460', 'c01461', 'c01462'],\n",
       "       ['c01470', 'c01471', 'c01472'],\n",
       "       ['c01480', 'c01481', 'c01482'],\n",
       "       ['c01490', 'c01491', 'c01492'],\n",
       "       ['c01500', 'c01501', 'c01502'],\n",
       "       ['c01510', 'c01511', 'c01512'],\n",
       "       ['c01520', 'c01521', 'c01522'],\n",
       "       ['c01530', 'c01531', 'c01532'],\n",
       "       ['c01540', 'c01541', 'c01542'],\n",
       "       ['c01550', 'c01551', 'c01552'],\n",
       "       ['c01560', 'c01561', 'c01562'],\n",
       "       ['c01570', 'c01571', 'c01572'],\n",
       "       ['c01580', 'c01581', 'c01582'],\n",
       "       ['c01590', 'c01591', 'c01592'],\n",
       "       ['c01600', 'c01601', 'c01602'],\n",
       "       ['c01610', 'c01611', 'c01612'],\n",
       "       ['c01620', 'c01621', 'c01622'],\n",
       "       ['c01630', 'c01631', 'c01632'],\n",
       "       ['c01640', 'c01641', 'c01642'],\n",
       "       ['c01650', 'c01651', 'c01652'],\n",
       "       ['c01660', 'c01661', 'c01662'],\n",
       "       ['c01670', 'c01671', 'c01672'],\n",
       "       ['c01680', 'c01681', 'c01682'],\n",
       "       ['c01690', 'c01691', 'c01692'],\n",
       "       ['c01700', 'c01701', 'c01702'],\n",
       "       ['c01710', 'c01711', 'c01712'],\n",
       "       ['c01720', 'c01721', 'c01722'],\n",
       "       ['c01730', 'c01731', 'c01732'],\n",
       "       ['c01740', 'c01741', 'c01742'],\n",
       "       ['c01750', 'c01751', 'c01752'],\n",
       "       ['c01760', 'c01761', 'c01762'],\n",
       "       ['c01770', 'c01771', 'c01772'],\n",
       "       ['c01780', 'c01781', 'c01782'],\n",
       "       ['c01790', 'c01791', 'c01792'],\n",
       "       ['c01800', 'c01801', 'c01802'],\n",
       "       ['c01810', 'c01811', 'c01812'],\n",
       "       ['c01820', 'c01821', 'c01822'],\n",
       "       ['c01830', 'c01831', 'c01832'],\n",
       "       ['c01840', 'c01841', 'c01842'],\n",
       "       ['c01850', 'c01851', 'c01852'],\n",
       "       ['c01860', 'c01861', 'c01862'],\n",
       "       ['c01870', 'c01871', 'c01872'],\n",
       "       ['c01880', 'c01881', 'c01882'],\n",
       "       ['c01890', 'c01891', 'c01892'],\n",
       "       ['c01900', 'c01901', 'c01902'],\n",
       "       ['c01910', 'c01911', 'c01912'],\n",
       "       ['c01920', 'c01921', 'c01922'],\n",
       "       ['c01930', 'c01931', 'c01932'],\n",
       "       ['c01940', 'c01941', 'c01942'],\n",
       "       ['c01950', 'c01951', 'c01952'],\n",
       "       ['c01960', 'c01961', 'c01962'],\n",
       "       ['c01970', 'c01971', 'c01972'],\n",
       "       ['c01980', 'c01981', 'c01982'],\n",
       "       ['c01990', 'c01991', 'c01992']],\n",
       "      dtype='|S6')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cin[0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "cin = [[[\"c{0}{1}{2}\".format(i, j, k) for k in xrange(3)]\n",
    "       for j in xrange(200)]\n",
    "       for i in xrange(batch_size)]\n",
    "cin = np.array(cin)\n",
    "\n",
    "dilation = 4\n",
    "\n",
    "encode_len = 100 + np.arange(batch_size)\n",
    "batch_idx = np.arange(batch_size)\n",
    "batch_idx = np.tile(batch_idx[:, np.newaxis], (1, dilation))\n",
    "\n",
    "qbt = encode_len - dilation - 1\n",
    "temporal_idx = qbt[:, np.newaxis] + np.arange(dilation).reshape(1, -1)\n",
    "\n",
    "idx = np.c_[batch_idx.reshape(-1, 1), temporal_idx.reshape(-1, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0],\n",
       "       [1, 1, 1, 1],\n",
       "       [2, 2, 2, 2],\n",
       "       [3, 3, 3, 3],\n",
       "       [4, 4, 4, 4],\n",
       "       [5, 5, 5, 5],\n",
       "       [6, 6, 6, 6],\n",
       "       [7, 7, 7, 7]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 95,  96,  97,  98],\n",
       "       [ 96,  97,  98,  99],\n",
       "       [ 97,  98,  99, 100],\n",
       "       [ 98,  99, 100, 101],\n",
       "       [ 99, 100, 101, 102],\n",
       "       [100, 101, 102, 103],\n",
       "       [101, 102, 103, 104],\n",
       "       [102, 103, 104, 105]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temporal_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0, 95],\n",
       "       [ 0, 96],\n",
       "       [ 0, 97],\n",
       "       [ 0, 98],\n",
       "       [ 1, 96],\n",
       "       [ 1, 97],\n",
       "       [ 1, 98],\n",
       "       [ 1, 99]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx[:batch_size, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================\n",
      "dilation = 4\n",
      "conv_input.shape = (8, 100, 32)\n",
      "batch_idx.shape = (32,)\n",
      "temporal_idx.shape = (32,)\n",
      "idx.shape = (32, 2)\n",
      "slices.shape = (32, 32)\n",
      "slices.shape = (8, 4, 32)\n"
     ]
    }
   ],
   "source": [
    "def print_shape(xs, name):\n",
    "    print \"{0}.shape = {1}\".format(name, xs.shape)\n",
    "\n",
    "# initialize state tensor arrays\n",
    "state_queues = []\n",
    "for i, (conv_input, dilation) in enumerate(zip(conv_inputs, [4])):\n",
    "    \n",
    "    print 75*\"=\"\n",
    "    print \"dilation = {0}\".format(dilation)\n",
    "    print_shape(conv_input, \"conv_input\")\n",
    "    \n",
    "    batch_idx = tf.range(batch_size)\n",
    "    batch_idx = tf.tile(tf.expand_dims(batch_idx, 1), (1, dilation))\n",
    "    batch_idx = tf.reshape(batch_idx, [-1])\n",
    "    print_shape(batch_idx, \"batch_idx\")\n",
    "    \n",
    "    queue_begin_time = self.encode_len - dilation - 1\n",
    "    temporal_idx = tf.expand_dims(queue_begin_time, 1) + tf.expand_dims(tf.range(dilation), 0)\n",
    "    temporal_idx = tf.reshape(temporal_idx, [-1])\n",
    "    print_shape(temporal_idx, \"temporal_idx\")\n",
    "\n",
    "    idx = tf.stack([batch_idx, temporal_idx], axis=1)\n",
    "    print_shape(idx, \"idx\")\n",
    "    slices = tf.gather_nd(conv_input, idx)\n",
    "    print_shape(slices, \"slices\")\n",
    "    slices = tf.reshape(slices, (batch_size, dilation, shape(conv_input, 2)))\n",
    "    print_shape(slices, \"slices\")\n",
    "    \n",
    "    layer_ta = tf.TensorArray(dtype=tf.float32, size=dilation + self.num_decode_steps)\n",
    "    layer_ta = layer_ta.unstack(tf.transpose(slices, (1, 0, 2)))\n",
    "    state_queues.append(layer_ta)\n",
    "\n",
    "# initialize feature tensor array\n",
    "features_ta = tf.TensorArray(dtype=tf.float32, size=self.num_decode_steps)\n",
    "features_ta = features_ta.unstack(tf.transpose(features, (1, 0, 2)))\n",
    "\n",
    "# initialize output tensor array\n",
    "emit_ta = tf.TensorArray(size=self.num_decode_steps, dtype=tf.float32)\n",
    "\n",
    "# initialize other loop vars\n",
    "elements_finished = 0 >= self.decode_len\n",
    "time = tf.constant(0, dtype=tf.int32)\n",
    "\n",
    "# get initial x input\n",
    "current_idx = tf.stack([tf.range(tf.shape(self.encode_len)[0]), self.encode_len - 1], axis=1)\n",
    "initial_input = tf.gather_nd(x, current_idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_outputs, updated_queues = [], []\n",
    "i = 0 \n",
    "time = 0\n",
    "current_input = initial_input\n",
    "queues = state_queues\n",
    "conv_input, queue, dilation = zip(conv_inputs, queues, self.dilations)[0]\n",
    "\n",
    "current_features = features_ta.read(time)\n",
    "current_input = tf.concat([current_input, current_features], axis=1)\n",
    "\n",
    "with tf.variable_scope('x-proj-decode', reuse=True):\n",
    "    w_x_proj = tf.get_variable('weights')\n",
    "    b_x_proj = tf.get_variable('biases')\n",
    "    x_proj = tf.nn.tanh(tf.matmul(current_input, w_x_proj) + b_x_proj)\n",
    "\n",
    "state = queue.read(time)\n",
    "with tf.variable_scope('dilated-conv-decode-{}'.format(i), reuse=True):\n",
    "    w_conv = tf.get_variable('weights'.format(i))\n",
    "    b_conv = tf.get_variable('biases'.format(i))\n",
    "    dilated_conv = tf.matmul(state, w_conv[0, :, :]) + tf.matmul(x_proj, w_conv[1, :, :]) + b_conv\n",
    "conv_filter, conv_gate = tf.split(dilated_conv, 2, axis=1)\n",
    "dilated_conv = tf.nn.tanh(conv_filter)*tf.nn.sigmoid(conv_gate)\n",
    "\n",
    "with tf.variable_scope('dilated-conv-proj-decode-{}'.format(i), reuse=True):\n",
    "    w_proj = tf.get_variable('weights'.format(i))\n",
    "    b_proj = tf.get_variable('biases'.format(i))\n",
    "    concat_outputs = tf.matmul(dilated_conv, w_proj) + b_proj\n",
    "skips, residuals = tf.split(concat_outputs, [self.skip_channels, self.residual_channels], axis=1)\n",
    "\n",
    "x_proj += residuals\n",
    "skip_outputs.append(skips)\n",
    "updated_queues.append(queue.write(time + dilation, x_proj))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "rchan = 1\n",
    "seq_len = 30\n",
    "dilation = 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>enc_out[0,0]</td>\n",
       "      <td>enc_out[0,1]</td>\n",
       "      <td>enc_out[0,2]</td>\n",
       "      <td>enc_out[0,3]</td>\n",
       "      <td>enc_out[0,4]</td>\n",
       "      <td>enc_out[0,5]</td>\n",
       "      <td>enc_out[0,6]</td>\n",
       "      <td>enc_out[0,7]</td>\n",
       "      <td>enc_out[0,8]</td>\n",
       "      <td>enc_out[0,9]</td>\n",
       "      <td>enc_out[0,10]</td>\n",
       "      <td>enc_out[0,11]</td>\n",
       "      <td>enc_out[0,12]</td>\n",
       "      <td>enc_out[0,13]</td>\n",
       "      <td>enc_out[0,14]</td>\n",
       "      <td>enc_out[0,15]</td>\n",
       "      <td>enc_out[0,16]</td>\n",
       "      <td>enc_out[0,17]</td>\n",
       "      <td>enc_out[0,18]</td>\n",
       "      <td>enc_out[0,19]</td>\n",
       "      <td>enc_out[0,20]</td>\n",
       "      <td>enc_out[0,21]</td>\n",
       "      <td>enc_out[0,22]</td>\n",
       "      <td>enc_out[0,23]</td>\n",
       "      <td>enc_out[0,24]</td>\n",
       "      <td>enc_out[0,25]</td>\n",
       "      <td>enc_out[0,26]</td>\n",
       "      <td>enc_out[0,27]</td>\n",
       "      <td>enc_out[0,28]</td>\n",
       "      <td>enc_out[0,29]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>enc_out[1,0]</td>\n",
       "      <td>enc_out[1,1]</td>\n",
       "      <td>enc_out[1,2]</td>\n",
       "      <td>enc_out[1,3]</td>\n",
       "      <td>enc_out[1,4]</td>\n",
       "      <td>enc_out[1,5]</td>\n",
       "      <td>enc_out[1,6]</td>\n",
       "      <td>enc_out[1,7]</td>\n",
       "      <td>enc_out[1,8]</td>\n",
       "      <td>enc_out[1,9]</td>\n",
       "      <td>enc_out[1,10]</td>\n",
       "      <td>enc_out[1,11]</td>\n",
       "      <td>enc_out[1,12]</td>\n",
       "      <td>enc_out[1,13]</td>\n",
       "      <td>enc_out[1,14]</td>\n",
       "      <td>enc_out[1,15]</td>\n",
       "      <td>enc_out[1,16]</td>\n",
       "      <td>enc_out[1,17]</td>\n",
       "      <td>enc_out[1,18]</td>\n",
       "      <td>enc_out[1,19]</td>\n",
       "      <td>enc_out[1,20]</td>\n",
       "      <td>enc_out[1,21]</td>\n",
       "      <td>enc_out[1,22]</td>\n",
       "      <td>enc_out[1,23]</td>\n",
       "      <td>enc_out[1,24]</td>\n",
       "      <td>enc_out[1,25]</td>\n",
       "      <td>enc_out[1,26]</td>\n",
       "      <td>enc_out[1,27]</td>\n",
       "      <td>enc_out[1,28]</td>\n",
       "      <td>enc_out[1,29]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>enc_out[2,0]</td>\n",
       "      <td>enc_out[2,1]</td>\n",
       "      <td>enc_out[2,2]</td>\n",
       "      <td>enc_out[2,3]</td>\n",
       "      <td>enc_out[2,4]</td>\n",
       "      <td>enc_out[2,5]</td>\n",
       "      <td>enc_out[2,6]</td>\n",
       "      <td>enc_out[2,7]</td>\n",
       "      <td>enc_out[2,8]</td>\n",
       "      <td>enc_out[2,9]</td>\n",
       "      <td>enc_out[2,10]</td>\n",
       "      <td>enc_out[2,11]</td>\n",
       "      <td>enc_out[2,12]</td>\n",
       "      <td>enc_out[2,13]</td>\n",
       "      <td>enc_out[2,14]</td>\n",
       "      <td>enc_out[2,15]</td>\n",
       "      <td>enc_out[2,16]</td>\n",
       "      <td>enc_out[2,17]</td>\n",
       "      <td>enc_out[2,18]</td>\n",
       "      <td>enc_out[2,19]</td>\n",
       "      <td>enc_out[2,20]</td>\n",
       "      <td>enc_out[2,21]</td>\n",
       "      <td>enc_out[2,22]</td>\n",
       "      <td>enc_out[2,23]</td>\n",
       "      <td>enc_out[2,24]</td>\n",
       "      <td>enc_out[2,25]</td>\n",
       "      <td>enc_out[2,26]</td>\n",
       "      <td>enc_out[2,27]</td>\n",
       "      <td>enc_out[2,28]</td>\n",
       "      <td>enc_out[2,29]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enc_out[3,0]</td>\n",
       "      <td>enc_out[3,1]</td>\n",
       "      <td>enc_out[3,2]</td>\n",
       "      <td>enc_out[3,3]</td>\n",
       "      <td>enc_out[3,4]</td>\n",
       "      <td>enc_out[3,5]</td>\n",
       "      <td>enc_out[3,6]</td>\n",
       "      <td>enc_out[3,7]</td>\n",
       "      <td>enc_out[3,8]</td>\n",
       "      <td>enc_out[3,9]</td>\n",
       "      <td>enc_out[3,10]</td>\n",
       "      <td>enc_out[3,11]</td>\n",
       "      <td>enc_out[3,12]</td>\n",
       "      <td>enc_out[3,13]</td>\n",
       "      <td>enc_out[3,14]</td>\n",
       "      <td>enc_out[3,15]</td>\n",
       "      <td>enc_out[3,16]</td>\n",
       "      <td>enc_out[3,17]</td>\n",
       "      <td>enc_out[3,18]</td>\n",
       "      <td>enc_out[3,19]</td>\n",
       "      <td>enc_out[3,20]</td>\n",
       "      <td>enc_out[3,21]</td>\n",
       "      <td>enc_out[3,22]</td>\n",
       "      <td>enc_out[3,23]</td>\n",
       "      <td>enc_out[3,24]</td>\n",
       "      <td>enc_out[3,25]</td>\n",
       "      <td>enc_out[3,26]</td>\n",
       "      <td>enc_out[3,27]</td>\n",
       "      <td>enc_out[3,28]</td>\n",
       "      <td>enc_out[3,29]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>enc_out[4,0]</td>\n",
       "      <td>enc_out[4,1]</td>\n",
       "      <td>enc_out[4,2]</td>\n",
       "      <td>enc_out[4,3]</td>\n",
       "      <td>enc_out[4,4]</td>\n",
       "      <td>enc_out[4,5]</td>\n",
       "      <td>enc_out[4,6]</td>\n",
       "      <td>enc_out[4,7]</td>\n",
       "      <td>enc_out[4,8]</td>\n",
       "      <td>enc_out[4,9]</td>\n",
       "      <td>enc_out[4,10]</td>\n",
       "      <td>enc_out[4,11]</td>\n",
       "      <td>enc_out[4,12]</td>\n",
       "      <td>enc_out[4,13]</td>\n",
       "      <td>enc_out[4,14]</td>\n",
       "      <td>enc_out[4,15]</td>\n",
       "      <td>enc_out[4,16]</td>\n",
       "      <td>enc_out[4,17]</td>\n",
       "      <td>enc_out[4,18]</td>\n",
       "      <td>enc_out[4,19]</td>\n",
       "      <td>enc_out[4,20]</td>\n",
       "      <td>enc_out[4,21]</td>\n",
       "      <td>enc_out[4,22]</td>\n",
       "      <td>enc_out[4,23]</td>\n",
       "      <td>enc_out[4,24]</td>\n",
       "      <td>enc_out[4,25]</td>\n",
       "      <td>enc_out[4,26]</td>\n",
       "      <td>enc_out[4,27]</td>\n",
       "      <td>enc_out[4,28]</td>\n",
       "      <td>enc_out[4,29]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>enc_out[5,0]</td>\n",
       "      <td>enc_out[5,1]</td>\n",
       "      <td>enc_out[5,2]</td>\n",
       "      <td>enc_out[5,3]</td>\n",
       "      <td>enc_out[5,4]</td>\n",
       "      <td>enc_out[5,5]</td>\n",
       "      <td>enc_out[5,6]</td>\n",
       "      <td>enc_out[5,7]</td>\n",
       "      <td>enc_out[5,8]</td>\n",
       "      <td>enc_out[5,9]</td>\n",
       "      <td>enc_out[5,10]</td>\n",
       "      <td>enc_out[5,11]</td>\n",
       "      <td>enc_out[5,12]</td>\n",
       "      <td>enc_out[5,13]</td>\n",
       "      <td>enc_out[5,14]</td>\n",
       "      <td>enc_out[5,15]</td>\n",
       "      <td>enc_out[5,16]</td>\n",
       "      <td>enc_out[5,17]</td>\n",
       "      <td>enc_out[5,18]</td>\n",
       "      <td>enc_out[5,19]</td>\n",
       "      <td>enc_out[5,20]</td>\n",
       "      <td>enc_out[5,21]</td>\n",
       "      <td>enc_out[5,22]</td>\n",
       "      <td>enc_out[5,23]</td>\n",
       "      <td>enc_out[5,24]</td>\n",
       "      <td>enc_out[5,25]</td>\n",
       "      <td>enc_out[5,26]</td>\n",
       "      <td>enc_out[5,27]</td>\n",
       "      <td>enc_out[5,28]</td>\n",
       "      <td>enc_out[5,29]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>enc_out[6,0]</td>\n",
       "      <td>enc_out[6,1]</td>\n",
       "      <td>enc_out[6,2]</td>\n",
       "      <td>enc_out[6,3]</td>\n",
       "      <td>enc_out[6,4]</td>\n",
       "      <td>enc_out[6,5]</td>\n",
       "      <td>enc_out[6,6]</td>\n",
       "      <td>enc_out[6,7]</td>\n",
       "      <td>enc_out[6,8]</td>\n",
       "      <td>enc_out[6,9]</td>\n",
       "      <td>enc_out[6,10]</td>\n",
       "      <td>enc_out[6,11]</td>\n",
       "      <td>enc_out[6,12]</td>\n",
       "      <td>enc_out[6,13]</td>\n",
       "      <td>enc_out[6,14]</td>\n",
       "      <td>enc_out[6,15]</td>\n",
       "      <td>enc_out[6,16]</td>\n",
       "      <td>enc_out[6,17]</td>\n",
       "      <td>enc_out[6,18]</td>\n",
       "      <td>enc_out[6,19]</td>\n",
       "      <td>enc_out[6,20]</td>\n",
       "      <td>enc_out[6,21]</td>\n",
       "      <td>enc_out[6,22]</td>\n",
       "      <td>enc_out[6,23]</td>\n",
       "      <td>enc_out[6,24]</td>\n",
       "      <td>enc_out[6,25]</td>\n",
       "      <td>enc_out[6,26]</td>\n",
       "      <td>enc_out[6,27]</td>\n",
       "      <td>enc_out[6,28]</td>\n",
       "      <td>enc_out[6,29]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>enc_out[7,0]</td>\n",
       "      <td>enc_out[7,1]</td>\n",
       "      <td>enc_out[7,2]</td>\n",
       "      <td>enc_out[7,3]</td>\n",
       "      <td>enc_out[7,4]</td>\n",
       "      <td>enc_out[7,5]</td>\n",
       "      <td>enc_out[7,6]</td>\n",
       "      <td>enc_out[7,7]</td>\n",
       "      <td>enc_out[7,8]</td>\n",
       "      <td>enc_out[7,9]</td>\n",
       "      <td>enc_out[7,10]</td>\n",
       "      <td>enc_out[7,11]</td>\n",
       "      <td>enc_out[7,12]</td>\n",
       "      <td>enc_out[7,13]</td>\n",
       "      <td>enc_out[7,14]</td>\n",
       "      <td>enc_out[7,15]</td>\n",
       "      <td>enc_out[7,16]</td>\n",
       "      <td>enc_out[7,17]</td>\n",
       "      <td>enc_out[7,18]</td>\n",
       "      <td>enc_out[7,19]</td>\n",
       "      <td>enc_out[7,20]</td>\n",
       "      <td>enc_out[7,21]</td>\n",
       "      <td>enc_out[7,22]</td>\n",
       "      <td>enc_out[7,23]</td>\n",
       "      <td>enc_out[7,24]</td>\n",
       "      <td>enc_out[7,25]</td>\n",
       "      <td>enc_out[7,26]</td>\n",
       "      <td>enc_out[7,27]</td>\n",
       "      <td>enc_out[7,28]</td>\n",
       "      <td>enc_out[7,29]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0             1             2             3             4   \\\n",
       "0  enc_out[0,0]  enc_out[0,1]  enc_out[0,2]  enc_out[0,3]  enc_out[0,4]   \n",
       "1  enc_out[1,0]  enc_out[1,1]  enc_out[1,2]  enc_out[1,3]  enc_out[1,4]   \n",
       "2  enc_out[2,0]  enc_out[2,1]  enc_out[2,2]  enc_out[2,3]  enc_out[2,4]   \n",
       "3  enc_out[3,0]  enc_out[3,1]  enc_out[3,2]  enc_out[3,3]  enc_out[3,4]   \n",
       "4  enc_out[4,0]  enc_out[4,1]  enc_out[4,2]  enc_out[4,3]  enc_out[4,4]   \n",
       "5  enc_out[5,0]  enc_out[5,1]  enc_out[5,2]  enc_out[5,3]  enc_out[5,4]   \n",
       "6  enc_out[6,0]  enc_out[6,1]  enc_out[6,2]  enc_out[6,3]  enc_out[6,4]   \n",
       "7  enc_out[7,0]  enc_out[7,1]  enc_out[7,2]  enc_out[7,3]  enc_out[7,4]   \n",
       "\n",
       "             5             6             7             8             9   \\\n",
       "0  enc_out[0,5]  enc_out[0,6]  enc_out[0,7]  enc_out[0,8]  enc_out[0,9]   \n",
       "1  enc_out[1,5]  enc_out[1,6]  enc_out[1,7]  enc_out[1,8]  enc_out[1,9]   \n",
       "2  enc_out[2,5]  enc_out[2,6]  enc_out[2,7]  enc_out[2,8]  enc_out[2,9]   \n",
       "3  enc_out[3,5]  enc_out[3,6]  enc_out[3,7]  enc_out[3,8]  enc_out[3,9]   \n",
       "4  enc_out[4,5]  enc_out[4,6]  enc_out[4,7]  enc_out[4,8]  enc_out[4,9]   \n",
       "5  enc_out[5,5]  enc_out[5,6]  enc_out[5,7]  enc_out[5,8]  enc_out[5,9]   \n",
       "6  enc_out[6,5]  enc_out[6,6]  enc_out[6,7]  enc_out[6,8]  enc_out[6,9]   \n",
       "7  enc_out[7,5]  enc_out[7,6]  enc_out[7,7]  enc_out[7,8]  enc_out[7,9]   \n",
       "\n",
       "              10             11             12             13             14  \\\n",
       "0  enc_out[0,10]  enc_out[0,11]  enc_out[0,12]  enc_out[0,13]  enc_out[0,14]   \n",
       "1  enc_out[1,10]  enc_out[1,11]  enc_out[1,12]  enc_out[1,13]  enc_out[1,14]   \n",
       "2  enc_out[2,10]  enc_out[2,11]  enc_out[2,12]  enc_out[2,13]  enc_out[2,14]   \n",
       "3  enc_out[3,10]  enc_out[3,11]  enc_out[3,12]  enc_out[3,13]  enc_out[3,14]   \n",
       "4  enc_out[4,10]  enc_out[4,11]  enc_out[4,12]  enc_out[4,13]  enc_out[4,14]   \n",
       "5  enc_out[5,10]  enc_out[5,11]  enc_out[5,12]  enc_out[5,13]  enc_out[5,14]   \n",
       "6  enc_out[6,10]  enc_out[6,11]  enc_out[6,12]  enc_out[6,13]  enc_out[6,14]   \n",
       "7  enc_out[7,10]  enc_out[7,11]  enc_out[7,12]  enc_out[7,13]  enc_out[7,14]   \n",
       "\n",
       "              15             16             17             18             19  \\\n",
       "0  enc_out[0,15]  enc_out[0,16]  enc_out[0,17]  enc_out[0,18]  enc_out[0,19]   \n",
       "1  enc_out[1,15]  enc_out[1,16]  enc_out[1,17]  enc_out[1,18]  enc_out[1,19]   \n",
       "2  enc_out[2,15]  enc_out[2,16]  enc_out[2,17]  enc_out[2,18]  enc_out[2,19]   \n",
       "3  enc_out[3,15]  enc_out[3,16]  enc_out[3,17]  enc_out[3,18]  enc_out[3,19]   \n",
       "4  enc_out[4,15]  enc_out[4,16]  enc_out[4,17]  enc_out[4,18]  enc_out[4,19]   \n",
       "5  enc_out[5,15]  enc_out[5,16]  enc_out[5,17]  enc_out[5,18]  enc_out[5,19]   \n",
       "6  enc_out[6,15]  enc_out[6,16]  enc_out[6,17]  enc_out[6,18]  enc_out[6,19]   \n",
       "7  enc_out[7,15]  enc_out[7,16]  enc_out[7,17]  enc_out[7,18]  enc_out[7,19]   \n",
       "\n",
       "              20             21             22             23             24  \\\n",
       "0  enc_out[0,20]  enc_out[0,21]  enc_out[0,22]  enc_out[0,23]  enc_out[0,24]   \n",
       "1  enc_out[1,20]  enc_out[1,21]  enc_out[1,22]  enc_out[1,23]  enc_out[1,24]   \n",
       "2  enc_out[2,20]  enc_out[2,21]  enc_out[2,22]  enc_out[2,23]  enc_out[2,24]   \n",
       "3  enc_out[3,20]  enc_out[3,21]  enc_out[3,22]  enc_out[3,23]  enc_out[3,24]   \n",
       "4  enc_out[4,20]  enc_out[4,21]  enc_out[4,22]  enc_out[4,23]  enc_out[4,24]   \n",
       "5  enc_out[5,20]  enc_out[5,21]  enc_out[5,22]  enc_out[5,23]  enc_out[5,24]   \n",
       "6  enc_out[6,20]  enc_out[6,21]  enc_out[6,22]  enc_out[6,23]  enc_out[6,24]   \n",
       "7  enc_out[7,20]  enc_out[7,21]  enc_out[7,22]  enc_out[7,23]  enc_out[7,24]   \n",
       "\n",
       "              25             26             27             28             29  \n",
       "0  enc_out[0,25]  enc_out[0,26]  enc_out[0,27]  enc_out[0,28]  enc_out[0,29]  \n",
       "1  enc_out[1,25]  enc_out[1,26]  enc_out[1,27]  enc_out[1,28]  enc_out[1,29]  \n",
       "2  enc_out[2,25]  enc_out[2,26]  enc_out[2,27]  enc_out[2,28]  enc_out[2,29]  \n",
       "3  enc_out[3,25]  enc_out[3,26]  enc_out[3,27]  enc_out[3,28]  enc_out[3,29]  \n",
       "4  enc_out[4,25]  enc_out[4,26]  enc_out[4,27]  enc_out[4,28]  enc_out[4,29]  \n",
       "5  enc_out[5,25]  enc_out[5,26]  enc_out[5,27]  enc_out[5,28]  enc_out[5,29]  \n",
       "6  enc_out[6,25]  enc_out[6,26]  enc_out[6,27]  enc_out[6,28]  enc_out[6,29]  \n",
       "7  enc_out[7,25]  enc_out[7,26]  enc_out[7,27]  enc_out[7,28]  enc_out[7,29]  "
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con_in = np.arange(batch_size*seq_len*rchan).reshape(batch_size, seq_len, rchan)\n",
    "con_in = np.array([[\"enc_out[{0},{1}]\".format(i, j) for j in xrange(seq_len)] for i in xrange(batch_size)])\n",
    "pd.DataFrame(con_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bi = np.arange(batch_size)\n",
    "bi = np.tile(bi.reshape(-1, 1), (1, dilation))\n",
    "bi = bi.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "el = 10 + np.arange(batch_size)\n",
    "qbt = el - dilation - 1\n",
    "ti = qbt.reshape(-1, 1) + np.arange(dilation)\n",
    "ti = ti.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.c_[bi, ti]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = np.array([con_in[i[0], i[1]] for i in idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = slices.reshape(batch_size, dilation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['enc_out[0,5]', 'enc_out[0,6]', 'enc_out[0,7]', 'enc_out[0,8]'],\n",
       "       ['enc_out[1,6]', 'enc_out[1,7]', 'enc_out[1,8]', 'enc_out[1,9]'],\n",
       "       ['enc_out[2,7]', 'enc_out[2,8]', 'enc_out[2,9]', 'enc_out[2,10]'],\n",
       "       ['enc_out[3,8]', 'enc_out[3,9]', 'enc_out[3,10]', 'enc_out[3,11]'],\n",
       "       ['enc_out[4,9]', 'enc_out[4,10]', 'enc_out[4,11]', 'enc_out[4,12]'],\n",
       "       ['enc_out[5,10]', 'enc_out[5,11]', 'enc_out[5,12]', 'enc_out[5,13]'],\n",
       "       ['enc_out[6,11]', 'enc_out[6,12]', 'enc_out[6,13]', 'enc_out[6,14]'],\n",
       "       ['enc_out[7,12]', 'enc_out[7,13]', 'enc_out[7,14]', 'enc_out[7,15]']],\n",
       "      dtype='|S13')"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 2)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0]])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.take(np.arange(9).reshape(3, 3), [0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2],\n",
       "       [3, 4, 5],\n",
       "       [6, 7, 8]])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.arange(9).reshape(3, 3)\n",
    "A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[(1, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
